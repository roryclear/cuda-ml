{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "weldEWJyQsol"
      },
      "outputs": [],
      "source": [
        "!pip install pycuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AqsIL2XiQ1A3"
      },
      "outputs": [],
      "source": [
        "import pycuda.compiler as comp\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "import numpy\n",
        "import time\n",
        "import math\n",
        "\n",
        "class Net():\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "    def setSize(self, layers):\n",
        "      self.layers = layers\n",
        "      self.numberOfNodes = 0\n",
        "      for i in range(len(self.layers)):\n",
        "        self.numberOfNodes += self.layers[i]\n",
        "\n",
        "      self.nodesGrad = numpy.zeros((self.numberOfNodes, 1),dtype=numpy.float32)\n",
        "      self.nodes = numpy.zeros((self.numberOfNodes, 1),dtype=numpy.float32)\n",
        "      self.loss = numpy.zeros((self.numberOfNodes, 1),dtype=numpy.float32)\n",
        "\n",
        "      self.numberOfWeights = 0\n",
        "      for i in range(len(self.layers)-1):\n",
        "        self.numberOfWeights += self.layers[i] * self.layers[i+1]\n",
        "\n",
        "      self.grads = numpy.zeros((self.numberOfWeights, 1),dtype=numpy.float32)\n",
        "      self.weights = numpy.zeros((self.numberOfWeights, 1),dtype=numpy.float32)\n",
        "      self.weightsLoss = numpy.zeros((self.numberOfWeights, 1),dtype=numpy.float32)\n",
        "\n",
        "    def copyToDevice(self):\n",
        "      self.weights_gpu = cuda.mem_alloc(self.weights.nbytes)\n",
        "      self.nodes_gpu = cuda.mem_alloc(self.nodes.nbytes)\n",
        "      self.grads_gpu = cuda.mem_alloc(self.grads.nbytes)\n",
        "      self.nodesGrad_gpu = cuda.mem_alloc(self.nodesGrad.nbytes)\n",
        "      self.loss_gpu = cuda.mem_alloc(self.loss.nbytes)\n",
        "\n",
        "      self.weightsLoss_gpu = cuda.mem_alloc(self.weightsLoss.nbytes)\n",
        "\n",
        "      cuda.memcpy_htod(self.weights_gpu,self.weights)\n",
        "      cuda.memcpy_htod(self.nodes_gpu,self.nodes)\n",
        "      cuda.memcpy_htod(self.grads_gpu,self.grads)\n",
        "      cuda.memcpy_htod(self.nodesGrad_gpu,self.nodesGrad)\n",
        "      cuda.memcpy_htod(self.loss_gpu,self.loss)\n",
        "\n",
        "      cuda.memcpy_htod(self.weightsLoss_gpu,self.weightsLoss)\n",
        "\n",
        "    def initWeights(self):\n",
        "      start = 0\n",
        "      for x in range(len(self.layers)-1):\n",
        "        numberOfWeights = self.layers[x] * self.layers[x+1]\n",
        "        layerSize = self.layers[x]\n",
        "        for y in range(numberOfWeights):\n",
        "          self.weights[start + y] = numpy.random.uniform() * (2 / numpy.sqrt(layerSize)) - 1 / numpy.sqrt(layerSize)\n",
        "        start += numberOfWeights\n",
        "\n",
        "    def free(self):\n",
        "      self.weights_gpu.free()\n",
        "      self.nodes_gpu.free()\n",
        "      self.grads_gpu.free()\n",
        "      self.nodesGrad_gpu.free()\n",
        "      self.loss_gpu.free()\n",
        "      self.weightsLoss_gpu.free()\n",
        "\n",
        "    def optimize(self):\n",
        "      length = len(self.weights)\n",
        "      bx,by,gx,gy = self.getBlockAndGridSize(length,1)\n",
        "      optimize(self.weights_gpu, self.grads_gpu,self.learningRate, numpy.int32(length), block=(bx,by,1),grid=(gx,gy))\n",
        "\n",
        "    def zero_grad(self):\n",
        "      length = len(self.weights)\n",
        "      bx,by,gx,gy = self.getBlockAndGridSize(length,1)\n",
        "      reset_values(self.grads_gpu,numpy.int32(length),block=(bx,by,1),grid=(gx,gy))\n",
        "  \n",
        "    def getLoss(self, answer):\n",
        "        numberOfLayers = len(self.layers)\n",
        "        start = numpy.int32(self.numberOfNodes - self.layers[numberOfLayers-1])\n",
        "        check_answer(training_correct_gpu, self.nodes_gpu, start, numpy.int32(answer),block=(1,1,1))\n",
        "\n",
        "        lengthx = self.layers[numberOfLayers-1]\n",
        "        lengthy = 1\n",
        "\n",
        "        bx,by,gx,gy = self.getBlockAndGridSize(lengthx,lengthy)\n",
        "        get_output_loss(self.loss_gpu, self.nodes_gpu, start, numpy.int32(answer),\n",
        "                        block=(bx,by,1),grid=(gx,gy))\n",
        "\n",
        "    def backward(self):\n",
        "      length = len(self.nodesGrad)\n",
        "\n",
        "      bx,by,gx,gy = self.getBlockAndGridSize(length,1)\n",
        "\n",
        "      der_sigmoid(self.nodesGrad_gpu,self.nodes_gpu, numpy.int32(length),block=(bx,by,1),grid=(gx,gy))\n",
        "      numberOfLayers = len(self.layers)\n",
        "\n",
        "      startw0 = numpy.int32(len(self.weights) - (self.layers[numberOfLayers-1] * self.layers[numberOfLayers-2]))\n",
        "      startn1 = numpy.int32(self.numberOfNodes - self.layers[numberOfLayers-1])\n",
        "      startn0 = startn1 - numpy.int32(self.layers[numberOfLayers-2])\n",
        "      lengthn0 = self.layers[numberOfLayers-2]\n",
        "      lengthn1 = self.layers[numberOfLayers-1]\n",
        "      lengthw1 = self.layers[numberOfLayers-2] * self.layers[numberOfLayers-1]\n",
        "      \n",
        "      lengthx = lengthn0\n",
        "      lengthy = lengthn1\n",
        "\n",
        "      bx,by,gx,gy = self.getBlockAndGridSize(lengthx,lengthy)\n",
        "\n",
        "      startC = startn0\n",
        "      startD = startw0\n",
        "      startA = startn1\n",
        "      startB = startA\n",
        "      ncB = numpy.int32(lengthn0)\n",
        "      nrA = numpy.int32(lengthn1)\n",
        "      multiply_them_index_add(self.grads_gpu, self.loss_gpu, self.nodesGrad_gpu,\n",
        "       self.nodes_gpu, startA, startB, startC, startD, ncB, nrA,\n",
        "        block=(bx,by,1), grid=(gx,gy)) \n",
        "      \n",
        "\n",
        "      startw1 = numpy.int32(len(self.weights))\n",
        "      for y in range(len(self.layers)-2):\n",
        "\n",
        "        startw1 -= numpy.int32(self.layers[numberOfLayers-1-y] * self.layers[numberOfLayers-2-y])\n",
        "        startw0 -= numpy.int32(self.layers[numberOfLayers-2-y] * self.layers[numberOfLayers-3-y])\n",
        "\n",
        "        startn1 -= numpy.int32(self.layers[numberOfLayers-2-y])\n",
        "        startn0 -= numpy.int32(self.layers[numberOfLayers-3-y])\n",
        "\n",
        "        lengthn0 = self.layers[numberOfLayers-3-y]\n",
        "        lengthn1 = self.layers[numberOfLayers-2-y]\n",
        "        lengthn2 = self.layers[numberOfLayers-1-y]\n",
        "\n",
        "        lengthw1 = self.layers[numberOfLayers-3-y] * self.layers[numberOfLayers-2-y]\n",
        "\n",
        "        length = lengthw1\n",
        "        bx,by,gx,gy = self.getBlockAndGridSize(length,1)\n",
        "        startD = startn1\n",
        "        startA = startw1\n",
        "        startB = startw1\n",
        "        array_mulitply(self.weightsLoss_gpu,self.weights_gpu,self.grads_gpu,startD,startA,startB,numpy.int32(length)\n",
        "        ,block=(bx,1,1),grid=(gx,1))\n",
        "\n",
        "        startA = startn1\n",
        "        length = lengthn1\n",
        "        startD = startn0\n",
        "        bx,by,gx,gy = self.getBlockAndGridSize(length,1)\n",
        "        numberOfNodesInLayer = numpy.int32(lengthn2)\n",
        "        get_node_loss(self.loss_gpu,self.weightsLoss_gpu,numberOfNodesInLayer,startA,startD,\n",
        "                      numpy.int32(length),block=(bx,1,1),grid=(gx,1))\n",
        "\n",
        "        startA = startn0\n",
        "        startB = startn1\n",
        "        startC = startn0\n",
        "        startD = startw0\n",
        "        lengthx = lengthn0\n",
        "        lengthy = lengthn1\n",
        "\n",
        "        bx,by,gx,gy = self.getBlockAndGridSize(lengthx,lengthy)\n",
        "\n",
        "        multiply_them_index_add(self.grads_gpu,self.loss_gpu,self.nodesGrad_gpu, self.nodes_gpu,startA,startB,startC,startD,numpy.int32(lengthx),numpy.int32(lengthy),\n",
        "                  block=(bx,by,1),grid=(gx,gy))        \n",
        "\n",
        "    def forward(self, input):\n",
        "      length = self.layers[0]\n",
        "\n",
        "      bx,by,gx,gy = self.getBlockAndGridSize(length,1)\n",
        "  \n",
        "      copy(self.nodes_gpu, input, numpy.int32(0), numpy.int32(0), numpy.int32(length), block=(bx,by,1), grid=(gx,gy))\n",
        "\n",
        "      startn0 = numpy.int32(0)\n",
        "      startn1 = numpy.int32(self.layers[0])\n",
        "      startw = numpy.int32(0)\n",
        "\n",
        "      for x in range(len(self.layers)-1):\n",
        "        \n",
        "        if x > 0:\n",
        "          startw += numpy.int32(self.layers[x-1] * self.layers[x])\n",
        "          startn1 += numpy.int32(self.layers[x])\n",
        "          startn0 += numpy.int32(self.layers[x-1])\n",
        "\n",
        "\n",
        "        n = self.layers[x]\n",
        "        n_NP = numpy.int32(n)\n",
        "        nrA = numpy.int32(self.layers[x+1])\n",
        "\n",
        "        bx,by,gx,gy = self.getBlockAndGridSize(1,self.layers[x+1])\n",
        "\n",
        "        multiply_them_index(self.nodes_gpu, self.weights_gpu, self.nodes_gpu, n_NP, numpy.int32(bx) \n",
        "        ,nrA , startn0, startn1,\n",
        "                              startw, block=(bx,by,1), grid=(gx,gy))\n",
        "\n",
        "        length = self.layers[x+1]\n",
        "        bx,by,gx,gy = self.getBlockAndGridSize(length,1)\n",
        "\n",
        "        sigmoid(self.nodes_gpu,startn1,numpy.int32(length),\n",
        "                      block=(bx,by,1), grid=(gx,gy))\n",
        "      return\n",
        "\n",
        "    def getBlockAndGridSize(self,lengthx,lengthy):\n",
        "      bx = lengthx\n",
        "      by = lengthy\n",
        "      gx = 1\n",
        "      gy = 1\n",
        "      if bx > MAX_THREADS_PER_BLOCK:\n",
        "        gx = math.ceil(bx / MAX_THREADS_PER_BLOCK)\n",
        "        bx = MAX_THREADS_PER_BLOCK\n",
        "\n",
        "      if by > MAX_THREADS_PER_BLOCK:\n",
        "        gy = math.ceil(by / MAX_THREADS_PER_BLOCK)\n",
        "        by = MAX_THREADS_PER_BLOCK\n",
        "\n",
        "      if bx * by > MAX_THREADS_PER_BLOCK:\n",
        "        if by > bx:\n",
        "          bx = math.floor(MAX_THREADS_PER_BLOCK / by)\n",
        "          gx = math.ceil(lengthx / bx)\n",
        "        else:\n",
        "          by = math.floor(MAX_THREADS_PER_BLOCK / bx)\n",
        "          gy = math.ceil(lengthy / by) \n",
        "      return bx,by,gx,gy\n",
        "\n",
        "mod = comp.SourceModule(\n",
        "    \"\"\"\n",
        "  __global__ void multiply_them_index(float *nodesD, float *weights, float *nodesA, int ncA, int ncB, int nrA, int startn0, int startD, int startW)\n",
        "{\n",
        "  int row = threadIdx.y + blockDim.y * blockIdx.y;\n",
        "  int col = threadIdx.x + blockDim.x * blockIdx.x;\n",
        "  float t = 0;\n",
        "  if(col < ncB && row < nrA)\n",
        "  {\n",
        "  for(int i = 0; i < ncA; i++){\n",
        "    t += weights[startW + (row * ncA) + i] * nodesA[startn0 + col + (i * ncB)];\n",
        "  }\n",
        "    nodesD[startD + (row * ncB) + col] = t;\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "__global__ void multiply_them_index_add(float *d, float *a, float *b ,float *c, int startA, int startB, int startC, int startD, int ncB, int nrA)\n",
        "{\n",
        "  int row = threadIdx.y + blockDim.y * blockIdx.y;\n",
        "  int col = threadIdx.x + blockDim.x * blockIdx.x;\n",
        "  if(col < ncB && row < nrA)\n",
        "  {\n",
        "    d[startD + (row * ncB) + col] += a[startA + row] * b[startB + row] * c[startC + col];\n",
        "  }\n",
        "}\n",
        "\n",
        "__global__ void optimize(float *d, float *a, float lr, int length)\n",
        "{\n",
        "  int i = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "  if(i < length)\n",
        "  {\n",
        "  d[i] = (lr * -a[i]) + d[i];\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "__global__ void array_mulitply(float *d, float *a, float *b, int startD, int startA, int startB, int length)\n",
        "{\n",
        "  const int i = threadIdx.x + (blockDim.x * blockIdx.x);\n",
        "  if(i < length)\n",
        "  {\n",
        "  d[startD + i] = a[startA + i] * b[startB + i];\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "__global__ void get_output_loss(float *d, float *o, int start, int a)\n",
        "{\n",
        "  int i = threadIdx.x;\n",
        "  if(i == a) {\n",
        "    d[start + i] = o[start + i] - 1;\n",
        "  } else {\n",
        "    d[start + i] = o[start + i];\n",
        "  }\n",
        "}\n",
        "\n",
        "__global__ void get_node_loss(float *d, float *a, int n, int startA, int startD, int length)\n",
        "{\n",
        "  int i = threadIdx.x + blockDim.x * blockIdx.x;\n",
        "  float t = 0;\n",
        "  for(int j = 0; j < n; j++) \n",
        "  {\n",
        "    if(i < length)\n",
        "    {\n",
        "    t += a[startA + i + j*length];\n",
        "    }\n",
        "  }\n",
        "  if(i < length)\n",
        "  { \n",
        "  d[startD + i] = t / n;\n",
        "  }\n",
        "}\n",
        "\n",
        "__global__ void reset_values(float *d, int length)\n",
        "{\n",
        "  int i = threadIdx.x + blockDim.x * blockIdx.x;\n",
        "  if(i < length)\n",
        "  {\n",
        "    d[i] = 0;\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "__global__ void check_answer(int *a, float *output, int start,int answer)\n",
        "{\n",
        "  for(int i = 0; i < 10; i++)\n",
        "  {\n",
        "    if(i != answer && output[start + i] >= output[start + answer])\n",
        "    {\n",
        "      return;\n",
        "    }\n",
        "  }\n",
        "  a[0] = a[0] + 1;\n",
        "}\n",
        "\n",
        "__global__ void sigmoid(float *d, int start, int length)\n",
        "{\n",
        "  const int i = threadIdx.x + blockDim.x * blockIdx.x;\n",
        "  if(i < length)\n",
        "  {\n",
        "    d[start + i] = 1 / (1 + exp(-d[start + i]));\n",
        "  }\n",
        "}\n",
        "\n",
        "__global__ void der_sigmoid(float *d, float *a, int length)\n",
        "{\n",
        "  const int i = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "  if(i < length)\n",
        "  {\n",
        "    d[i] = a[i] * (1 - a[i]);\n",
        "  }\n",
        "}\n",
        "\n",
        "__global__ void copy(float *d, float *a, int startA, int startD, int length)\n",
        "{\n",
        "  int i = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "  if(i < length)\n",
        "  {\n",
        "    d[i + startD] = a[i + startA];\n",
        "  }\n",
        "}\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "MAX_THREADS_PER_BLOCK = \\\n",
        "    cuda.Device(0).get_attribute(pycuda._driver.device_attribute.MAX_THREADS_PER_BLOCK)\n",
        "\n",
        "multiply_them_index = mod.get_function(\"multiply_them_index\")\n",
        "multiply_them_index_add = mod.get_function(\"multiply_them_index_add\")\n",
        "optimize = mod.get_function(\"optimize\")\n",
        "sigmoid = mod.get_function(\"sigmoid\")\n",
        "der_sigmoid = mod.get_function(\"der_sigmoid\")\n",
        "array_mulitply = mod.get_function(\"array_mulitply\")\n",
        "get_output_loss = mod.get_function(\"get_output_loss\")\n",
        "get_node_loss = mod.get_function(\"get_node_loss\")\n",
        "reset_values = mod.get_function(\"reset_values\")\n",
        "check_answer = mod.get_function(\"check_answer\")\n",
        "copy = mod.get_function(\"copy\")\n",
        "\n",
        "\n",
        "training_correct = numpy.zeros((1),dtype=numpy.int32)\n",
        "training_correct_gpu = cuda.mem_alloc(training_correct.nbytes)\n",
        "cuda.memcpy_htod(training_correct_gpu,training_correct)\n",
        "\n",
        "test_correct = numpy.zeros((1),dtype=numpy.int32)\n",
        "test_correct_gpu = cuda.mem_alloc(test_correct.nbytes)\n",
        "cuda.memcpy_htod(test_correct_gpu,test_correct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kz_SvjWcQ4uy"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "transform=transforms.Compose([\n",
        "transforms.ToTensor()\n",
        "])\n",
        "\n",
        "trainDataset = datasets.MNIST('../data', train=True, download=True,\n",
        "                transform=transform)\n",
        "testDataset = datasets.MNIST('../data', train=False,\n",
        "                transform=transform)\n",
        "\n",
        "img_train = trainDataset.data.numpy()\n",
        "img_train = img_train / 255\n",
        "img_train = numpy.float32(img_train)\n",
        "\n",
        "img_test = testDataset.data.numpy()\n",
        "img_test = img_test / 255\n",
        "img_test = numpy.float32(img_test)\n",
        "\n",
        "label_train2 = trainDataset.targets\n",
        "\n",
        "trainImg32 = img_train[0].astype(numpy.float32)\n",
        "img_gpu = cuda.mem_alloc(trainImg32.nbytes)\n",
        "\n",
        "cudaNet = Net()\n",
        "\n",
        "cudaNet.setSize([784,256,10])\n",
        "cudaNet.initWeights()\n",
        "cudaNet.learningRate = numpy.float32(1.0)\n",
        "cudaNet.copyToDevice()\n",
        "\n",
        "batchSize = 1\n",
        "for epoch in range(1):\n",
        "  print(\"\\nEPOCH\",epoch,\"\\n\")\n",
        "  start_time = time.time()\n",
        "  for i in range(len(img_train)): \n",
        "    trainImg32 = img_train[i].astype(numpy.float32)\n",
        "    cuda.memcpy_htod(img_gpu,trainImg32)\n",
        "\n",
        "    cudaNet.forward(img_gpu)\n",
        "    cudaNet.getLoss(label_train2[i])\n",
        "    cudaNet.backward()\n",
        "    \n",
        "    if i % batchSize == 0 or i == (len(img_train) - 1):\n",
        "      cudaNet.optimize()      \n",
        "      cudaNet.zero_grad()  \n",
        "\n",
        "  print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "  cuda.memcpy_dtoh(training_correct,training_correct_gpu)\n",
        "  reset_values(training_correct_gpu,numpy.int32(1),block=(1,1,1))\n",
        "  print(\"train dataset: correct = \",(training_correct[0]/len(img_train)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rhedg26RSv2M"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class PytorchNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PytorchNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 256, False)\n",
        "        self.fc2 = nn.Linear(256, 10, False)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.sigmoid(x)\n",
        "        x = self.fc2(x)\n",
        "        return F.sigmoid(x)\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            targetOutput = torch.tensor([0,0,0,0,0,0,0,0,0,0])\n",
        "            targetOutput[target[0].item()] = 1\n",
        "            targetOutput = targetOutput.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.smooth_l1_loss(output, targetOutput)\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    \n",
        "torch.manual_seed(1)\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "transform=transforms.Compose([\n",
        "transforms.ToTensor()\n",
        "])\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(testDataset)\n",
        "\n",
        "model = PytorchNet().to(device)\n",
        "\n",
        "# copy weights from cudaNet\n",
        "cuda.memcpy_dtoh(cudaNet.weights,cudaNet.weights_gpu)\n",
        "start = 0\n",
        "i = 0\n",
        "for fc in model.children():\n",
        "  with torch.no_grad():\n",
        "    for y in range(cudaNet.layers[i]):\n",
        "      for x in range(cudaNet.layers[i+1]):\n",
        "        fc.weight[x, y] = torch.cuda.FloatTensor(cudaNet.weights[start + y + x*cudaNet.layers[i]])\n",
        "    start += cudaNet.layers[i] * cudaNet.layers[i+1]\n",
        "    i += 1\n",
        "\n",
        "cudaNet.free()\n",
        "test(model, device, test_loader)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}