{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiEsn8gJJlmx"
      },
      "outputs": [],
      "source": [
        "!pip install pycuda\n",
        "!pip install tensorflow #only for getting data\n",
        "!pip install ipytest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "w4PYMj9Sa4VI"
      },
      "outputs": [],
      "source": [
        "import pycuda.compiler as comp\n",
        "import pycuda.driver as cuda\n",
        "import numpy\n",
        "import pycuda.autoinit\n",
        "import time\n",
        "import math\n",
        "from tensorflow import keras\n",
        "from os.path import exists\n",
        "\n",
        "#pip install pycuda\n",
        "#pip install tensorflow\n",
        "\n",
        "class Net():\n",
        "    #This defines the structure of the NN.\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "    def setSize(self, layers):\n",
        "      self.layers = layers\n",
        "      self.numberOfNodes = 0\n",
        "      for i in range(len(self.layers)):\n",
        "        self.numberOfNodes += self.layers[i]\n",
        "\n",
        "      self.nodesInput = numpy.zeros((self.numberOfNodes, 1),dtype=numpy.float32)\n",
        "      self.nodes = numpy.zeros((self.numberOfNodes, 1),dtype=numpy.float32)\n",
        "      self.loss = numpy.zeros((self.numberOfNodes, 1),dtype=numpy.float32)\n",
        "\n",
        "      self.numberOfWeights = 0\n",
        "      for i in range(len(self.layers)-1):\n",
        "        self.numberOfWeights += self.layers[i] * self.layers[i+1]\n",
        "\n",
        "      self.grads = numpy.zeros((self.numberOfWeights, 1),dtype=numpy.float32)\n",
        "      self.weights = numpy.zeros((self.numberOfWeights, 1),dtype=numpy.float32)\n",
        "\n",
        "    def copyToDevice(self):\n",
        "      self.weights_gpu = cuda.mem_alloc(self.weights.nbytes)\n",
        "      cuda.memcpy_htod(self.weights_gpu,self.weights)\n",
        "      self.nodes_gpu = cuda.mem_alloc(self.nodes.nbytes)\n",
        "      cuda.memcpy_htod(self.nodes_gpu,self.nodes)\n",
        "      self.grads_gpu = cuda.mem_alloc(self.grads.nbytes)\n",
        "      cuda.memcpy_htod(self.grads_gpu,self.grads)\n",
        "      self.nodesInput_gpu = cuda.mem_alloc(self.nodesInput.nbytes)\n",
        "      cuda.memcpy_htod(self.nodesInput_gpu,self.nodesInput)\n",
        "      self.loss_gpu = cuda.mem_alloc(self.loss.nbytes)\n",
        "      cuda.memcpy_htod(self.loss_gpu,self.loss)\n",
        "\n",
        "    def loadWeights(self, path):\n",
        "      weightsFile = path\n",
        "      for i in range(len(self.layers) - 1):\n",
        "        weightsFile += str(self.layers[i]) + \"-\"\n",
        "      weightsFile += str(self.layers[len(self.layers)-1]) + \".txt\"\n",
        "      if exists(weightsFile):\n",
        "        f = open(weightsFile, \"r\")\n",
        "        lines = f.readlines()\n",
        "        for i in range(len(lines)):\n",
        "          line = lines[i].replace(\"\\n\",\"\")\n",
        "          self.weights[i] = line\n",
        "\n",
        "      else:\n",
        "        print(\"no weights file was found\")    \n",
        "        for x in range(len(self.weights)):\n",
        "          self.weights[x] = numpy.random.uniform() * (2 / numpy.sqrt(self.layers[0])) - 1 / numpy.sqrt(self.layers[0])\n",
        "\n",
        "    def optimize(self):\n",
        "      length = len(self.weights)\n",
        "      bx,by,gx,gy = self.getBlockAndGridSize(length,1)\n",
        "      optimize(self.weights_gpu, self.grads_gpu,self.learningRate, numpy.int32(length), block=(bx,by,1),grid=(gx,gy))\n",
        "\n",
        "    def zero_grad(self):\n",
        "      length = len(self.weights)\n",
        "      bx,by,gx,gy = self.getBlockAndGridSize(length,1)\n",
        "      reset_values(self.grads_gpu,numpy.int32(length),block=(bx,by,1),grid=(gx,gy))\n",
        "  \n",
        "    def backward(self):\n",
        "      length = len(self.nodesInput)\n",
        "\n",
        "      bx,by,gx,gy = self.getBlockAndGridSize(length,1)\n",
        "\n",
        "      der_sigmoid(self.nodesInput_gpu,self.nodes_gpu, numpy.int32(length),block=(bx,by,1),grid=(gx,gy))\n",
        "\n",
        "      startw1 = numpy.int32(len(self.weights) - (self.layers[2] * self.layers[1]))\n",
        "      startw0 = numpy.int32(startw1 - (self.layers[1] * self.layers[0]))\n",
        "      startn2 = numpy.int32(self.numberOfNodes - self.layers[2])\n",
        "      startn1 = startn2 - numpy.int32(self.layers[1])\n",
        "      startn0 = startn1 - numpy.int32(self.layers[0])\n",
        "      lengthn0 = self.layers[0]\n",
        "      lengthn1 = self.layers[1]\n",
        "      lengthn2 = self.layers[2]\n",
        "      lengthw0 = self.layers[0] * self.layers[1]\n",
        "      lengthw1 = self.layers[1] * self.layers[2]\n",
        "\n",
        "      ###---------------------------\n",
        "\n",
        "      start = startn2\n",
        "      check_answer(training_correct_gpu, self.nodes_gpu, start, numpy.int32(label_train[i]),block=(1,1,1))\n",
        "\n",
        "      #backward\n",
        "      start = startn2\n",
        "      lengthx = lengthn2\n",
        "      lengthy = 1\n",
        "\n",
        "      bx,by,gx,gy = self.getBlockAndGridSize(lengthx,lengthy)\n",
        "\n",
        "      get_output_loss(self.loss_gpu, self.nodes_gpu, start, numpy.int32(label_train[i]),\n",
        "                      block=(bx,by,1),grid=(gx,gy))\n",
        "      \n",
        "      lengthx = lengthn1\n",
        "      lengthy = lengthn2\n",
        "\n",
        "      bx,by,gx,gy = self.getBlockAndGridSize(lengthx,lengthy)\n",
        "\n",
        "      #int ncA, int ncB, int nrA\n",
        "      startC = startn1\n",
        "      startD = startw1\n",
        "      startA = startn2\n",
        "      startB = startA\n",
        "      ncB = numpy.int32(lengthn1)\n",
        "      nrA = numpy.int32(lengthn2)\n",
        "      #__global__ void multiply_them_index_minus(float *d, float *a, float *b ,float *c, int startA, int startB, int startC, int startD, int ncB, int nrA)\n",
        "      multiply_them_index_add(self.grads_gpu, self.loss_gpu, self.nodesInput_gpu,\n",
        "       self.nodes_gpu, startA, startB, startC, startD, ncB, nrA,\n",
        "        block=(bx,by,1), grid=(gx,gy)) \n",
        "      \n",
        "      #backward first weights ???\n",
        "      length = lengthw1\n",
        "      bx = length\n",
        "      gx = 1\n",
        "      if bx > MAX_THREADS_PER_BLOCK:\n",
        "        gx = int(bx / MAX_THREADS_PER_BLOCK) + 1\n",
        "        bx = MAX_THREADS_PER_BLOCK\n",
        "      startD = startn1\n",
        "      startA = startw1\n",
        "      startB = startw1\n",
        "      array_mulitply(self.loss_gpu,self.weights_gpu,self.grads_gpu,startD,startA,startB,numpy.int32(length)\n",
        "      ,block=(bx,1,1),grid=(gx,1))\n",
        "\n",
        "      startA = startn1\n",
        "      length = lengthn1\n",
        "      bx = length\n",
        "      gx = 1\n",
        "      if bx > MAX_THREADS_PER_BLOCK:\n",
        "        gx = int(bx / MAX_THREADS_PER_BLOCK) + 1\n",
        "        bx = MAX_THREADS_PER_BLOCK\n",
        "      numberOfNodesInLayer = numpy.int32(lengthn2)\n",
        "      get_node_loss(self.loss_gpu,numberOfNodesInLayer,startA,\n",
        "                    numpy.int32(length),block=(bx,1,1),grid=(gx,1))\n",
        "\n",
        "      startA = startn0\n",
        "      startB = startn1\n",
        "      startC = startn0\n",
        "      startD = startw0\n",
        "\n",
        "      lengthx = lengthn0\n",
        "      lengthy = lengthn1\n",
        "\n",
        "      bx,by,gx,gy = self.getBlockAndGridSize(lengthx,lengthy)\n",
        "\n",
        "      multiply_them_index_add(self.grads_gpu,self.loss_gpu,self.nodesInput_gpu, self.nodes_gpu,startA,startB,startC,startD,numpy.int32(lengthx),numpy.int32(lengthy),\n",
        "                block=(bx,by,1),grid=(gx,gy))\n",
        "\n",
        "    def forward(self):\n",
        "\n",
        "      #copy input (n0_gpu) to nodes_gpu\n",
        "      length = self.layers[0]\n",
        "\n",
        "      bx,by,gx,gy = self.getBlockAndGridSize(length,1)\n",
        "  \n",
        "      copy(self.nodes_gpu, img_gpu, numpy.int32(0), numpy.int32(0), numpy.int32(length), block=(bx,by,1), grid=(gx,gy))\n",
        "\n",
        "      startn0 = numpy.int32(0)\n",
        "      startn1 = numpy.int32(self.layers[0])\n",
        "      startw = numpy.int32(0)\n",
        "      start = numpy.int32(0)\n",
        "      for x in range(len(self.layers)-1):\n",
        "        \n",
        "        if x > 0:\n",
        "          startw += numpy.int32(self.layers[x-1] * self.layers[x])\n",
        "          startn1 += numpy.int32(self.layers[x])\n",
        "          startn0 += numpy.int32(self.layers[x-1])\n",
        "\n",
        "\n",
        "        n = self.layers[x] # number of columns in A / number of rows in B\n",
        "        n_NP = numpy.int32(n)\n",
        "        nrA = numpy.int32(self.layers[x+1])\n",
        "\n",
        "        bx,by,gx,gy = self.getBlockAndGridSize(1,self.layers[x+1]) # number of cols in B, number of rows in A\n",
        "\n",
        "        multiply_them_index(self.nodes_gpu, self.weights_gpu, self.nodes_gpu, n_NP, numpy.int32(bx) \n",
        "        ,nrA , startn0, startn1,\n",
        "                              startw, block=(bx,by,1), grid=(gx,gy))\n",
        "\n",
        "        length = self.layers[x+1]\n",
        "        start += numpy.int32(self.layers[x])\n",
        "\n",
        "        bx,by,gx,gy = self.getBlockAndGridSize(length,1)\n",
        "\n",
        "        sigmoid_index(self.nodes_gpu,start,numpy.int32(length),\n",
        "                      block=(bx,by,1), grid=(gx,gy))\n",
        "\n",
        "\n",
        "      return 0\n",
        "\n",
        "    def getBlockAndGridSize(self,lengthx,lengthy):\n",
        "      bx = lengthx\n",
        "      by = lengthy\n",
        "      gx = 1\n",
        "      gy = 1\n",
        "      if bx > MAX_THREADS_PER_BLOCK:\n",
        "        gx = math.ceil(bx / MAX_THREADS_PER_BLOCK)\n",
        "        bx = MAX_THREADS_PER_BLOCK\n",
        "\n",
        "      if by > MAX_THREADS_PER_BLOCK:\n",
        "        gy = math.ceil(by / MAX_THREADS_PER_BLOCK)\n",
        "        by = MAX_THREADS_PER_BLOCK\n",
        "\n",
        "      if bx * by > MAX_THREADS_PER_BLOCK:\n",
        "        if by > bx:\n",
        "          bx = math.ceil(MAX_THREADS_PER_BLOCK / by)\n",
        "          gx = math.ceil(lengthx / bx)\n",
        "        else:\n",
        "          by = int(MAX_THREADS_PER_BLOCK / bx)\n",
        "          gy = math.ceil(lengthy / by) \n",
        "      return bx,by,gx,gy\n",
        "\n",
        "mod = comp.SourceModule(\n",
        "    \"\"\"\n",
        "  __global__ void multiply_them_index(float *nodesD, float *weights, float *nodesA, int ncA, int ncB, int nrA, int startn0, int startD, int startW)\n",
        "{\n",
        "  int row = threadIdx.y + blockDim.y * blockIdx.y;\n",
        "  int col = threadIdx.x + blockDim.x * blockIdx.x;\n",
        "  float t = 0;\n",
        "  if(col < ncB && row < nrA)\n",
        "  {\n",
        "  for(int i = 0; i < ncA; i++){\n",
        "    t += weights[startW + (row * ncA) + i] * nodesA[startn0 + col + (i * ncB)];\n",
        "  }\n",
        "    nodesD[startD + (row * ncB) + col] = t;\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "__global__ void multiply_them_index_add(float *d, float *a, float *b ,float *c, int startA, int startB, int startC, int startD, int ncB, int nrA)\n",
        "{\n",
        "  int row = threadIdx.y + blockDim.y * blockIdx.y;\n",
        "  int col = threadIdx.x + blockDim.x * blockIdx.x;\n",
        "  if(col < ncB && row < nrA)\n",
        "  {\n",
        "    d[startD + (row * ncB) + col] += a[startA + row] * b[startB + row] * c[startC + col];\n",
        "  }\n",
        "}\n",
        "\n",
        "__global__ void optimize(float *d, float *a, float lr, int length)\n",
        "{\n",
        "  int i = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "  if(i < length)\n",
        "  {\n",
        "  d[i] = (lr * -a[i]) + d[i];\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "__global__ void array_mulitply(float *d, float *a, float *b, int startD, int startA, int startB, int length)\n",
        "{\n",
        "  const int i = threadIdx.x + (blockDim.x * blockIdx.x);\n",
        "  if(i < length)\n",
        "  {\n",
        "  d[startD + i] = a[startA + i] * b[startB + i];\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "__global__ void get_output_loss(float *d, float *o, int start, int a)\n",
        "{\n",
        "  int i = threadIdx.x;\n",
        "  if(i == a) {\n",
        "    d[start + i] = o[start + i] - 1;\n",
        "  } else {\n",
        "    d[start + i] = o[start + i];\n",
        "  }\n",
        "}\n",
        "\n",
        "__global__ void get_node_loss(float *d, int n, int startA, int length)\n",
        "{\n",
        "  int i = threadIdx.x + blockDim.x * blockIdx.x;\n",
        "  float t = 0;\n",
        "  for(int j = 0; j < n; j++) \n",
        "  {\n",
        "    if(i < length)\n",
        "    {\n",
        "    t += d[startA + i + j*length];\n",
        "    }\n",
        "  }\n",
        "  if(i < length)\n",
        "  { \n",
        "  d[i] = t / n;\n",
        "  }\n",
        "}\n",
        "\n",
        "__global__ void reset_values(float *d, int length)\n",
        "{\n",
        "  int i = threadIdx.x + blockDim.x * blockIdx.x;\n",
        "  if(i < length)\n",
        "  {\n",
        "    d[i] = 0;\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "__global__ void check_answer(int *a, float *output, int start,int answer)\n",
        "{\n",
        "  for(int i = 0; i < 10; i++)\n",
        "  {\n",
        "    if(output[start + i] > output[start + answer])\n",
        "    {\n",
        "      return;\n",
        "    }\n",
        "  }\n",
        "  a[0] = a[0] + 1;\n",
        "}\n",
        "\n",
        "__global__ void sigmoid_index(float *d, int start, int length)\n",
        "{\n",
        "  const int i = threadIdx.x + blockDim.x * blockIdx.x;\n",
        "  if(i < length)\n",
        "  {\n",
        "    d[start + i] = 1 / (1 + exp(-d[start + i]));\n",
        "  }\n",
        "}\n",
        "\n",
        "__global__ void der_sigmoid(float *d, float *a, int length)\n",
        "{\n",
        "  const int i = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "  if(i < length)\n",
        "  {\n",
        "    d[i] = a[i] * (1 - a[i]);\n",
        "  }\n",
        "}\n",
        "\n",
        "__global__ void copy(float *d, float *a, int startA, int startD, int length)\n",
        "{\n",
        "  int i = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "  if(i < length)\n",
        "  {\n",
        "    d[i + startD] = a[i + startA];\n",
        "  }\n",
        "}\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "MAX_THREADS_PER_BLOCK = \\\n",
        "    cuda.Device(0).get_attribute(pycuda._driver.device_attribute.MAX_THREADS_PER_BLOCK)\n",
        "\n",
        "multiply_them_index = mod.get_function(\"multiply_them_index\")\n",
        "multiply_them_index_add = mod.get_function(\"multiply_them_index_add\") #adds to result\n",
        "optimize = mod.get_function(\"optimize\")\n",
        "sigmoid_index = mod.get_function(\"sigmoid_index\")\n",
        "der_sigmoid = mod.get_function(\"der_sigmoid\")\n",
        "array_mulitply = mod.get_function(\"array_mulitply\")\n",
        "get_output_loss = mod.get_function(\"get_output_loss\")\n",
        "get_node_loss = mod.get_function(\"get_node_loss\")\n",
        "reset_values = mod.get_function(\"reset_values\")\n",
        "check_answer = mod.get_function(\"check_answer\")\n",
        "copy = mod.get_function(\"copy\")\n",
        "\n",
        "def test(testNet):\n",
        "  reset_values(test_correct_gpu,numpy.int32(1),block=(1,1,1))\n",
        "  start_time = time.time()\n",
        "  start = numpy.int32(0)\n",
        "  for x in range(len(testNet.layers)-1):\n",
        "    start += numpy.int32(testNet.layers[x])\n",
        "  for i in range(len(img_test)):\n",
        "    testImg32 = img_test[i].astype(numpy.float32)  \n",
        "    cuda.memcpy_htod(img_gpu, testImg32)\n",
        "\n",
        "    testNet.forward()\n",
        "    check_answer(test_correct_gpu, testNet.nodes_gpu, start, numpy.int32(label_test[i]),block=(1,1,1))\n",
        "  print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "  cuda.memcpy_dtoh(test_correct,test_correct_gpu)\n",
        "  print(\"test dataset: correct = \",(test_correct[0]/len(img_test)))\n",
        "\n",
        "#---- mnist stuff ---- \n",
        "\n",
        "(img_train, label_train), (img_test, label_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "img_train = img_train / 255\n",
        "img_test = img_test / 255\n",
        "\n",
        "training_correct = numpy.zeros((1),dtype=numpy.int32)\n",
        "training_correct_gpu = cuda.mem_alloc(training_correct.nbytes)\n",
        "cuda.memcpy_htod(training_correct_gpu,training_correct)\n",
        "\n",
        "test_correct = numpy.zeros((1),dtype=numpy.int32)\n",
        "test_correct_gpu = cuda.mem_alloc(test_correct.nbytes)\n",
        "cuda.memcpy_htod(test_correct_gpu,test_correct)\n",
        "\n",
        "trainImg32 = img_train[0].astype(numpy.float32)\n",
        "img_gpu = cuda.mem_alloc(trainImg32.nbytes)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testNet = Net()\n",
        "\n",
        "testNet.setSize([784,16,10]) #backward function limited to 2 layers atm\n",
        "\n",
        "#weightsFile = \"sigmoid-weights\"\n",
        "weightsFile = \"sigmoid-untrained-weights\"\n",
        "testNet.loadWeights(weightsFile)\n",
        "testNet.learningRate = numpy.float32(0.1)\n",
        "testNet.copyToDevice()\n",
        "\n",
        "batchSize = 1\n",
        "for epoch in range(1):\n",
        "  print(\"\\nEPOCH\",epoch,\"\\n\")\n",
        "  start_time = time.time()\n",
        "  for i in range(len(img_train)): \n",
        "    trainImg32 = img_train[i].astype(numpy.float32)\n",
        "    cuda.memcpy_htod(img_gpu,trainImg32)\n",
        "\n",
        "    testNet.forward()\n",
        "\n",
        "    testNet.backward()\n",
        "    \n",
        "    if i % batchSize == 0 or i == (len(img_train) - 1):\n",
        "      testNet.optimize()      \n",
        "      testNet.zero_grad()  \n",
        "\n",
        "  print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "  cuda.memcpy_dtoh(training_correct,training_correct_gpu)\n",
        "  reset_values(training_correct_gpu,numpy.int32(1),block=(1,1,1))\n",
        "  print(\"train dataset: correct = \",(training_correct[0]/len(img_train)))\n",
        "  test(testNet)\n",
        "  assert test_correct[0]/len(img_test) == 0.8948,\"test accuracy has changed.\"\n",
        "\n",
        "testNet = Net()\n",
        "testNet.setSize([784,4,10]) #backward function limited to 2 layers atm\n",
        "#weightsFile = \"sigmoid-weights\"\n",
        "weightsFile = \"sigmoid-untrained-weights\"\n",
        "testNet.loadWeights(weightsFile)\n",
        "testNet.learningRate = numpy.float32(0.1)\n",
        "testNet.copyToDevice()\n",
        "\n",
        "\n",
        "batchSize = 1\n",
        "for epoch in range(1):\n",
        "  print(\"\\nEPOCH\",epoch,\"\\n\")\n",
        "  start_time = time.time()\n",
        "  for i in range(len(img_train)): \n",
        "    trainImg32 = img_train[i].astype(numpy.float32)\n",
        "    cuda.memcpy_htod(img_gpu,trainImg32)\n",
        "\n",
        "    testNet.forward()\n",
        "\n",
        "    testNet.backward()\n",
        "    \n",
        "    if i % batchSize == 0 or i == (len(img_train) - 1):\n",
        "      testNet.optimize()      \n",
        "      testNet.zero_grad()  \n",
        "\n",
        "  print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "  cuda.memcpy_dtoh(training_correct,training_correct_gpu)\n",
        "  reset_values(training_correct_gpu,numpy.int32(1),block=(1,1,1))\n",
        "  print(\"train dataset: correct = \",(training_correct[0]/len(img_train)))\n",
        "  test(testNet)\n",
        "  assert test_correct[0]/len(img_test) == 0.7046,\"test accuracy has changed.\"\n",
        "\n",
        "\n",
        "testNet = Net()\n",
        "testNet.learningRate = numpy.float32(0.1)\n",
        "testNet.setSize([784,1200,10]) #backward function limited to 2 layers atm\n",
        "#weightsFile = \"sigmoid-weights\"\n",
        "testNet.loadWeights(\"sigmoid-untrained-weights\")\n",
        "testNet.copyToDevice()\n",
        "\n",
        "\n",
        "for epoch in range(1):\n",
        "  print(\"\\nEPOCH\",epoch,\"\\n\")\n",
        "  start_time = time.time()\n",
        "  for i in range(10000): \n",
        "    trainImg32 = img_train[i].astype(numpy.float32)\n",
        "    cuda.memcpy_htod(img_gpu,trainImg32)\n",
        "\n",
        "    testNet.forward()\n",
        "\n",
        "    testNet.backward()\n",
        "    \n",
        "    if i % batchSize == 0 or i == (len(img_train) - 1):\n",
        "      testNet.optimize()      \n",
        "      testNet.zero_grad()  \n",
        "\n",
        "  print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "  cuda.memcpy_dtoh(training_correct,training_correct_gpu)\n",
        "  reset_values(training_correct_gpu,numpy.int32(1),block=(1,1,1))\n",
        "  print(\"train dataset: correct = \",(training_correct[0]/len(img_train)))\n",
        "  test(testNet)\n",
        "  assert test_correct[0]/len(img_test) == 0.593,\"test accuracy has changed.\""
      ],
      "metadata": {
        "id": "cjbZ9Crl8w6D"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}