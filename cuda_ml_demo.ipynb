{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "weldEWJyQsol"
      },
      "outputs": [],
      "source": [
        "!pip install pycuda"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class and functions for training:"
      ],
      "metadata": {
        "id": "BSqgEnGbw4Y8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqsIL2XiQ1A3"
      },
      "outputs": [],
      "source": [
        "import pycuda.compiler as comp\n",
        "import pycuda.driver as cuda\n",
        "import numpy\n",
        "import pycuda.autoinit\n",
        "import time\n",
        "import math\n",
        "from tensorflow import keras\n",
        "from os.path import exists\n",
        "\n",
        "class Net():\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "    def setSize(self, layers):\n",
        "      self.layers = layers\n",
        "      self.numberOfNodes = 0\n",
        "      for i in range(len(self.layers)):\n",
        "        self.numberOfNodes += self.layers[i]\n",
        "\n",
        "      self.nodesGrad = numpy.zeros((self.numberOfNodes, 1),dtype=numpy.float32)\n",
        "      self.nodes = numpy.zeros((self.numberOfNodes, 1),dtype=numpy.float32)\n",
        "      self.loss = numpy.zeros((self.numberOfNodes, 1),dtype=numpy.float32)\n",
        "\n",
        "      self.numberOfWeights = 0\n",
        "      for i in range(len(self.layers)-1):\n",
        "        self.numberOfWeights += self.layers[i] * self.layers[i+1]\n",
        "\n",
        "      self.grads = numpy.zeros((self.numberOfWeights, 1),dtype=numpy.float32)\n",
        "      self.weights = numpy.zeros((self.numberOfWeights, 1),dtype=numpy.float32)\n",
        "      self.weightsLoss = numpy.zeros((self.numberOfWeights, 1),dtype=numpy.float32)\n",
        "\n",
        "    def copyToDevice(self):\n",
        "      self.weights_gpu = cuda.mem_alloc(self.weights.nbytes)\n",
        "      self.nodes_gpu = cuda.mem_alloc(self.nodes.nbytes)\n",
        "      self.grads_gpu = cuda.mem_alloc(self.grads.nbytes)\n",
        "      self.nodesGrad_gpu = cuda.mem_alloc(self.nodesGrad.nbytes)\n",
        "      self.loss_gpu = cuda.mem_alloc(self.loss.nbytes)\n",
        "\n",
        "      self.weightsLoss_gpu = cuda.mem_alloc(self.weightsLoss.nbytes)\n",
        "\n",
        "      cuda.memcpy_htod(self.weights_gpu,self.weights)\n",
        "      cuda.memcpy_htod(self.nodes_gpu,self.nodes)\n",
        "      cuda.memcpy_htod(self.grads_gpu,self.grads)\n",
        "      cuda.memcpy_htod(self.nodesGrad_gpu,self.nodesGrad)\n",
        "      cuda.memcpy_htod(self.loss_gpu,self.loss)\n",
        "\n",
        "      cuda.memcpy_htod(self.weightsLoss_gpu,self.weightsLoss)\n",
        "\n",
        "\n",
        "    def loadWeights(self, path):\n",
        "      weightsFile = path\n",
        "      for i in range(len(self.layers) - 1):\n",
        "        weightsFile += str(self.layers[i]) + \"-\"\n",
        "      weightsFile += str(self.layers[len(self.layers)-1]) + \".txt\"\n",
        "      if exists(weightsFile):\n",
        "        f = open(weightsFile, \"r\")\n",
        "        lines = f.readlines()\n",
        "        for i in range(len(lines)):\n",
        "          line = lines[i].replace(\"\\n\",\"\")\n",
        "          self.weights[i] = line\n",
        "      else:\n",
        "        print(\"\\nno weights file was found\")\n",
        "        start = 0\n",
        "        for x in range(len(self.layers)-1):\n",
        "          numberOfWeights = self.layers[x] * self.layers[x+1]\n",
        "          layerSize = self.layers[x]\n",
        "          for y in range(numberOfWeights):\n",
        "            self.weights[start + y] = numpy.random.uniform() * (2 / numpy.sqrt(layerSize)) - 1 / numpy.sqrt(layerSize)\n",
        "          start += numberOfWeights\n",
        "\n",
        "    def initWeights(self):\n",
        "      start = 0\n",
        "      for x in range(len(self.layers)-1):\n",
        "        numberOfWeights = self.layers[x] * self.layers[x+1]\n",
        "        layerSize = self.layers[x]\n",
        "        for y in range(numberOfWeights):\n",
        "          self.weights[start + y] = numpy.random.uniform() * (2 / numpy.sqrt(layerSize)) - 1 / numpy.sqrt(layerSize)\n",
        "        start += numberOfWeights\n",
        "\n",
        "    def free(self):\n",
        "      self.weights_gpu.free()\n",
        "      self.nodes_gpu.free()\n",
        "      self.grads_gpu.free()\n",
        "      self.nodesGrad_gpu.free()\n",
        "      self.loss_gpu.free()\n",
        "      self.weightsLoss_gpu.free()\n",
        "\n",
        "    def optimize(self):\n",
        "      ops = len(self.weights)\n",
        "      bx = min(ops,MAX_THREADS_PER_BLOCK)\n",
        "      gx = math.ceil(ops / MAX_THREADS_PER_BLOCK)\n",
        "      optimize(self.weights_gpu, self.grads_gpu,self.learningRate, numpy.int32(ops), block=(bx,1,1),grid=(gx,1))\n",
        "\n",
        "    def zero_grad(self):\n",
        "      ops = len(self.weights)\n",
        "      bx = min(ops,MAX_THREADS_PER_BLOCK)\n",
        "      gx = math.ceil(ops / MAX_THREADS_PER_BLOCK)\n",
        "      reset_values(self.grads_gpu,numpy.int32(ops),block=(bx,1,1),grid=(gx,1))\n",
        "  \n",
        "    def getLoss(self, answer):\n",
        "        numberOfLayers = len(self.layers)\n",
        "        start = numpy.int32(self.numberOfNodes - self.layers[numberOfLayers-1])\n",
        "\n",
        "        ops = self.layers[numberOfLayers-1]\n",
        "        bx = min(ops,MAX_THREADS_PER_BLOCK)\n",
        "        gx = math.ceil(ops / MAX_THREADS_PER_BLOCK)\n",
        "        get_output_loss(self.loss_gpu, self.nodes_gpu, start, numpy.int32(answer),\n",
        "                        block=(bx,1,1),grid=(gx,1))\n",
        "\n",
        "    def backward(self):\n",
        "      ops = len(self.nodesGrad)\n",
        "      bx = min(ops,MAX_THREADS_PER_BLOCK)\n",
        "      gx = math.ceil(ops / MAX_THREADS_PER_BLOCK)\n",
        "\n",
        "      der_sigmoid(self.nodesGrad_gpu,self.nodes_gpu, numpy.int32(ops),block=(bx,1,1),grid=(gx,1))\n",
        "      numberOfLayers = len(self.layers)\n",
        "\n",
        "      startw0 = numpy.int32(len(self.weights) - (self.layers[numberOfLayers-1] * self.layers[numberOfLayers-2]))\n",
        "      startn1 = numpy.int32(self.numberOfNodes - self.layers[numberOfLayers-1])\n",
        "      startn0 = startn1 - numpy.int32(self.layers[numberOfLayers-2])\n",
        "      lengthn0 = self.layers[numberOfLayers-2]\n",
        "      lengthn1 = self.layers[numberOfLayers-1]\n",
        "      lengthw1 = self.layers[numberOfLayers-2] * self.layers[numberOfLayers-1]\n",
        "      \n",
        "      lengthx = lengthn0\n",
        "      lengthy = lengthn1\n",
        "\n",
        "      bx,by,gx,gy = self.getBlockAndGridSize(lengthx,lengthy)\n",
        "\n",
        "      startC = startn0\n",
        "      startD = startw0\n",
        "      startA = startn1\n",
        "      startB = startA\n",
        "      ncB = numpy.int32(lengthn0)\n",
        "      nrA = numpy.int32(lengthn1)\n",
        "      multiply_them_3(self.grads_gpu, self.loss_gpu, self.nodesGrad_gpu,\n",
        "       self.nodes_gpu, startA, startB, startC, startD, ncB, nrA,\n",
        "        block=(bx,by,1), grid=(gx,gy))\n",
        "      \n",
        "      startw1 = numpy.int32(len(self.weights))\n",
        "      for y in range(len(self.layers)-2):\n",
        "\n",
        "        startw1 -= numpy.int32(self.layers[numberOfLayers-1-y] * self.layers[numberOfLayers-2-y])\n",
        "        startw0 -= numpy.int32(self.layers[numberOfLayers-2-y] * self.layers[numberOfLayers-3-y])\n",
        "\n",
        "        startn1 -= numpy.int32(self.layers[numberOfLayers-2-y])\n",
        "        startn0 -= numpy.int32(self.layers[numberOfLayers-3-y])\n",
        "\n",
        "        lengthn0 = self.layers[numberOfLayers-3-y]\n",
        "        lengthn1 = self.layers[numberOfLayers-2-y]\n",
        "        lengthn2 = self.layers[numberOfLayers-1-y]\n",
        "\n",
        "        lengthw1 = self.layers[numberOfLayers-3-y] * self.layers[numberOfLayers-2-y]\n",
        "\n",
        "        length = lengthw1 #todo dont think we need this?\n",
        "        ops = lengthw1\n",
        "        bx = min(ops,MAX_THREADS_PER_BLOCK)\n",
        "        gx = math.ceil(ops / MAX_THREADS_PER_BLOCK)\n",
        "        startD = startn1\n",
        "        startA = startw1\n",
        "        startB = startw1\n",
        "        array_mulitply(self.weightsLoss_gpu,self.weights_gpu,self.grads_gpu,startD,startA,startB,numpy.int32(lengthw1)\n",
        "        ,block=(bx,1,1),grid=(gx,1))\n",
        "\n",
        "        startA = startn1\n",
        "        length = lengthn1\n",
        "        startD = startn0\n",
        "\n",
        "        ops = lengthw1\n",
        "        bx = min(ops,MAX_THREADS_PER_BLOCK)\n",
        "        gx = math.ceil(ops / MAX_THREADS_PER_BLOCK)\n",
        "\n",
        "        get_node_loss(self.loss_gpu,self.weightsLoss_gpu,numpy.int32(lengthn2),startA,startD,\n",
        "                      numpy.int32(length),block=(bx,1,1),grid=(gx,1))\n",
        "\n",
        "        startA = startn0\n",
        "        startB = startn1\n",
        "        startC = startn0\n",
        "        startD = startw0\n",
        "\n",
        "        lengthx = lengthn0\n",
        "        lengthy = lengthn1\n",
        "\n",
        "        bx,by,gx,gy = self.getBlockAndGridSize(lengthx,lengthy)\n",
        "\n",
        "        multiply_them_3(self.grads_gpu,self.loss_gpu,self.nodesGrad_gpu, self.nodes_gpu,startA,startB,startC,startD,numpy.int32(lengthx),numpy.int32(lengthy),\n",
        "                  block=(bx,by,1),grid=(gx,gy))\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "      #copy input (n0_gpu) to nodes_gpu\n",
        "      length = self.layers[0]\n",
        "\n",
        "      bx,by,gx,gy = self.getBlockAndGridSize(length,1)\n",
        "  \n",
        "      copy(self.nodes_gpu, input, numpy.int32(0), numpy.int32(0), numpy.int32(length), block=(bx,by,1), grid=(gx,gy))\n",
        "\n",
        "      startn0 = numpy.int32(0)\n",
        "      startn1 = numpy.int32(self.layers[0])\n",
        "      startw = numpy.int32(0)\n",
        "      for x in range(len(self.layers)-1):\n",
        "        \n",
        "        if x > 0:\n",
        "          startw += numpy.int32(self.layers[x-1] * self.layers[x])\n",
        "          startn1 += numpy.int32(self.layers[x])\n",
        "          startn0 += numpy.int32(self.layers[x-1])\n",
        "\n",
        "\n",
        "        n = self.layers[x] # number of columns in A / number of rows in B\n",
        "        n_NP = numpy.int32(n)\n",
        "        nrA = numpy.int32(self.layers[x+1])\n",
        "\n",
        "        bx,by,gx,gy = self.getBlockAndGridSize(1,self.layers[x+1]) # number of cols in B, number of rows in A\n",
        "\n",
        "        multiply_them(self.nodes_gpu, self.weights_gpu, self.nodes_gpu, n_NP, numpy.int32(bx)\n",
        "        ,nrA , startn0, startn1,\n",
        "                              startw, block=(bx,by,1), grid=(gx,gy))\n",
        "\n",
        "        ops = self.layers[x+1]\n",
        "        bx = min(ops,MAX_THREADS_PER_BLOCK)\n",
        "        gx = math.ceil(ops / MAX_THREADS_PER_BLOCK)\n",
        "\n",
        "        sigmoid(self.nodes_gpu,startn1,numpy.int32(ops),\n",
        "                      block=(bx,1,1), grid=(gx,1))\n",
        "      return\n",
        "\n",
        "    def getBlockAndGridSize(self,lengthx,lengthy):\n",
        "      bx = lengthx\n",
        "      by = lengthy\n",
        "      gx = 1\n",
        "      gy = 1\n",
        "      if bx > MAX_THREADS_PER_BLOCK:\n",
        "        gx = math.ceil(bx / MAX_THREADS_PER_BLOCK)\n",
        "        bx = MAX_THREADS_PER_BLOCK\n",
        "\n",
        "      if by > MAX_THREADS_PER_BLOCK:\n",
        "        gy = math.ceil(by / MAX_THREADS_PER_BLOCK)\n",
        "        by = MAX_THREADS_PER_BLOCK\n",
        "\n",
        "      if bx * by > MAX_THREADS_PER_BLOCK:\n",
        "        if by > bx:\n",
        "          bx = math.floor(MAX_THREADS_PER_BLOCK / by)\n",
        "          gx = math.ceil(lengthx / bx)\n",
        "        else:\n",
        "          by = math.floor(MAX_THREADS_PER_BLOCK / bx)\n",
        "          gy = math.ceil(lengthy / by)\n",
        "      return bx,by,gx,gy\n",
        "\n",
        "mod = comp.SourceModule(\n",
        "    \"\"\"\n",
        "  __global__ void multiply_them(float *nodesD, float *weights, float *nodesA, int ncA, int ncB, int nrA, int startn0, int startD, int startW)\n",
        "{\n",
        "  int row = threadIdx.y + blockDim.y * blockIdx.y;\n",
        "  int col = threadIdx.x + blockDim.x * blockIdx.x;\n",
        "  float t = 0;\n",
        "  if(col < ncB && row < nrA)\n",
        "  {\n",
        "  for(int i = 0; i < ncA; i++){\n",
        "    t += weights[startW + (row * ncA) + i] * nodesA[startn0 + col + (i * ncB)];\n",
        "  }\n",
        "    nodesD[startD + (row * ncB) + col] = t;\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "__global__ void multiply_them_3(float *d, float *a, float *b ,float *c, int startA, int startB, int startC, int startD, int ncB, int nrA)\n",
        "{\n",
        "  int row = threadIdx.y + blockDim.y * blockIdx.y;\n",
        "  int col = threadIdx.x + blockDim.x * blockIdx.x;\n",
        "  if(col < ncB && row < nrA)\n",
        "  {\n",
        "    d[startD + (row * ncB) + col] += a[startA + row] * b[startB + row] * c[startC + col];\n",
        "  }\n",
        "}\n",
        "\n",
        "__global__ void optimize(float *d, float *a, float lr, int length)\n",
        "{\n",
        "  int i = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "  if(i < length)\n",
        "  {\n",
        "  d[i] = (lr * -a[i]) + d[i];\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "__global__ void array_mulitply(float *d, float *a, float *b, int startD, int startA, int startB, int length)\n",
        "{\n",
        "  const int i = threadIdx.x + (blockDim.x * blockIdx.x);\n",
        "  if(i < length)\n",
        "  {\n",
        "  d[startD + i] = a[startA + i] * b[startB + i];\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "__global__ void get_output_loss(float *d, float *o, int start, int a)\n",
        "{\n",
        "  int i = threadIdx.x;\n",
        "  if(i == a) {\n",
        "    d[start + i] = o[start + i] - 1;\n",
        "  } else {\n",
        "    d[start + i] = o[start + i];\n",
        "  }\n",
        "}\n",
        "\n",
        "__global__ void get_node_loss(float *d, float *a, int n, int startA, int startD, int length)\n",
        "{\n",
        "  int i = threadIdx.x + blockDim.x * blockIdx.x;\n",
        "  float t = 0;\n",
        "  for(int j = 0; j < n; j++)\n",
        "  {\n",
        "    if(i < length)\n",
        "    {\n",
        "    t += a[startA + i + j*length];\n",
        "    }\n",
        "  }\n",
        "  if(i < length)\n",
        "  {\n",
        "  d[startD + i] = t / n;\n",
        "  }\n",
        "}\n",
        "\n",
        "__global__ void reset_values(float *d, int length)\n",
        "{\n",
        "  int i = threadIdx.x + blockDim.x * blockIdx.x;\n",
        "  if(i < length)\n",
        "  {\n",
        "    d[i] = 0;\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "__global__ void check_answer(int *a, float *output, int start,int answer)\n",
        "{\n",
        "  for(int i = 0; i < 10; i++)\n",
        "  {\n",
        "    if(i != answer && output[start + i] >= output[start + answer])\n",
        "    {\n",
        "      return;\n",
        "    }\n",
        "  }\n",
        "  a[0] = a[0] + 1;\n",
        "}\n",
        "\n",
        "__global__ void sigmoid(float *d, int start, int length)\n",
        "{\n",
        "  const int i = threadIdx.x + blockDim.x * blockIdx.x;\n",
        "  if(i < length)\n",
        "  {\n",
        "    d[start + i] = 1 / (1 + exp(-d[start + i]));\n",
        "  }\n",
        "}\n",
        "\n",
        "__global__ void der_sigmoid(float *d, float *a, int length)\n",
        "{\n",
        "  const int i = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "  if(i < length)\n",
        "  {\n",
        "    d[i] = a[i] * (1 - a[i]);\n",
        "  }\n",
        "}\n",
        "\n",
        "__global__ void copy(float *d, float *a, int startA, int startD, int length)\n",
        "{\n",
        "  int i = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "  if(i < length)\n",
        "  {\n",
        "    d[i + startD] = a[i + startA];\n",
        "  }\n",
        "}\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "MAX_THREADS_PER_BLOCK = \\\n",
        "    cuda.Device(0).get_attribute(pycuda._driver.device_attribute.MAX_THREADS_PER_BLOCK)\n",
        "\n",
        "multiply_them = mod.get_function(\"multiply_them\")\n",
        "multiply_them_3 = mod.get_function(\"multiply_them_3\")\n",
        "optimize = mod.get_function(\"optimize\")\n",
        "sigmoid = mod.get_function(\"sigmoid\")\n",
        "der_sigmoid = mod.get_function(\"der_sigmoid\")\n",
        "array_mulitply = mod.get_function(\"array_mulitply\")\n",
        "get_output_loss = mod.get_function(\"get_output_loss\")\n",
        "get_node_loss = mod.get_function(\"get_node_loss\")\n",
        "reset_values = mod.get_function(\"reset_values\")\n",
        "check_answer = mod.get_function(\"check_answer\")\n",
        "copy = mod.get_function(\"copy\")\n",
        "\n",
        "def test(testNet):\n",
        "  reset_values(test_correct_gpu,numpy.int32(1),block=(1,1,1))\n",
        "  start_time = time.time()\n",
        "  start = numpy.int32(0)\n",
        "  for x in range(len(testNet.layers)-1):\n",
        "    start += numpy.int32(testNet.layers[x])\n",
        "  for i in range(len(img_test)):\n",
        "    testImg32 = img_test[i].astype(numpy.float32)\n",
        "    cuda.memcpy_htod(img_gpu, testImg32)\n",
        "\n",
        "    testNet.forward(img_gpu)\n",
        "    check_answer(test_correct_gpu, testNet.nodes_gpu, start, numpy.int32(label_test[i]),block=(1,1,1))\n",
        "  print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "  cuda.memcpy_dtoh(test_correct,test_correct_gpu)\n",
        "  print(\"test dataset: correct = \",(test_correct[0]/len(img_test)))\n",
        "\n",
        "def checkTrainingAnswer(testNet, answer):\n",
        "  numberOfLayers = len(testNet.layers)\n",
        "  start = numpy.int32(testNet.numberOfNodes - testNet.layers[numberOfLayers-1])\n",
        "  check_answer(training_correct_gpu, testNet.nodes_gpu, start, numpy.int32(answer),block=(1,1,1))\n",
        "\n",
        "#---- mnist stuff ----\n",
        "\n",
        "(img_train, label_train), (img_test, label_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "img_train = img_train / 255\n",
        "img_test = img_test / 255\n",
        "\n",
        "training_correct = numpy.zeros((1),dtype=numpy.int32)\n",
        "training_correct_gpu = cuda.mem_alloc(training_correct.nbytes)\n",
        "cuda.memcpy_htod(training_correct_gpu,training_correct)\n",
        "\n",
        "test_correct = numpy.zeros((1),dtype=numpy.int32)\n",
        "test_correct_gpu = cuda.mem_alloc(test_correct.nbytes)\n",
        "cuda.memcpy_htod(test_correct_gpu,test_correct)\n",
        "\n",
        "trainImg32 = img_train[0].astype(numpy.float32)\n",
        "img_gpu = cuda.mem_alloc(trainImg32.nbytes)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the net:"
      ],
      "metadata": {
        "id": "tN_Tsmh_xPcL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kz_SvjWcQ4uy"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "transform=transforms.Compose([\n",
        "transforms.ToTensor()\n",
        "])\n",
        "\n",
        "trainDataset = datasets.MNIST('../data', train=True, download=True,\n",
        "                transform=transform)\n",
        "testDataset = datasets.MNIST('../data', train=False,\n",
        "                transform=transform)\n",
        "\n",
        "img_train = trainDataset.data.numpy()\n",
        "img_train = img_train / 255\n",
        "img_train = numpy.float32(img_train)\n",
        "\n",
        "img_test = testDataset.data.numpy()\n",
        "img_test = img_test / 255\n",
        "img_test = numpy.float32(img_test)\n",
        "\n",
        "label_train = trainDataset.targets\n",
        "\n",
        "trainImg32 = img_train[0].astype(numpy.float32)\n",
        "img_gpu = cuda.mem_alloc(trainImg32.nbytes)\n",
        "\n",
        "cudaNet = Net()\n",
        "\n",
        "cudaNet.setSize([784,256,10])\n",
        "cudaNet.initWeights()\n",
        "cudaNet.learningRate = numpy.float32(1.0)\n",
        "cudaNet.copyToDevice()\n",
        "\n",
        "batchSize = 1\n",
        "\n",
        "numberOfLayers = len(cudaNet.layers)\n",
        "outputStart = numpy.int32(cudaNet.numberOfNodes - cudaNet.layers[numberOfLayers-1])\n",
        "\n",
        "for epoch in range(1):\n",
        "  print(\"\\nEPOCH\",epoch,\"\\n\")\n",
        "  start_time = time.time()\n",
        "  for i in range(len(img_train)): \n",
        "    trainImg32 = img_train[i].astype(numpy.float32)\n",
        "    cuda.memcpy_htod(img_gpu,trainImg32)\n",
        "\n",
        "    cudaNet.forward(img_gpu)\n",
        "    cudaNet.getLoss(label_train[i])\n",
        "    check_answer(training_correct_gpu, cudaNet.nodes_gpu, outputStart, numpy.int32(label_train[i]),block=(1,1,1))\n",
        "    cudaNet.backward()\n",
        "    \n",
        "    if i % batchSize == 0 or i == (len(img_train) - 1):\n",
        "      cudaNet.optimize()      \n",
        "      cudaNet.zero_grad()  \n",
        "\n",
        "  print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "  cuda.memcpy_dtoh(training_correct,training_correct_gpu)\n",
        "  reset_values(training_correct_gpu,numpy.int32(1),block=(1,1,1))\n",
        "  print(\"train dataset: correct = \",(training_correct[0]/len(img_train)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy weights to pytorch and test on test dataset:"
      ],
      "metadata": {
        "id": "PlIMfhRixWdF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rhedg26RSv2M"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class PytorchNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PytorchNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 256, False)\n",
        "        self.fc2 = nn.Linear(256, 10, False)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = torch.sigmoid(x)\n",
        "        x = self.fc2(x)\n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "def testPytorch(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            targetOutput = torch.tensor([[0,0,0,0,0,0,0,0,0,0]])\n",
        "            targetOutput[0][target[0].item()] = 1\n",
        "            targetOutput = targetOutput.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.smooth_l1_loss(output, targetOutput)\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    \n",
        "torch.manual_seed(1)\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "transform=transforms.Compose([\n",
        "transforms.ToTensor()\n",
        "])\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(testDataset)\n",
        "\n",
        "model = PytorchNet().to(device)\n",
        "\n",
        "# copy weights from cudaNet\n",
        "cuda.memcpy_dtoh(cudaNet.weights,cudaNet.weights_gpu)\n",
        "start = 0\n",
        "i = 0\n",
        "for fc in model.children():\n",
        "  with torch.no_grad():\n",
        "    for y in range(cudaNet.layers[i]):\n",
        "      for x in range(cudaNet.layers[i+1]):\n",
        "        fc.weight[x, y] = torch.cuda.FloatTensor(cudaNet.weights[start + y + x*cudaNet.layers[i]])\n",
        "    start += cudaNet.layers[i] * cudaNet.layers[i+1]\n",
        "    i += 1\n",
        "\n",
        "cudaNet.free()\n",
        "testPytorch(model, device, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1N1klS99qdZ4JbzK4VvfQYQe5dvzY3Emz\n",
        "!gdown 1qG-JighJzx_8t2X7K_0RbJDw-K3nDNGr\n",
        "!gdown 1g1FkE7XIR4_DA9dZ-tx-Eb5PMS7yUVIt\n",
        "!gdown 1xRxAOh5I6WOdUAw3SMWHUfNU4bRKgnD8"
      ],
      "metadata": {
        "id": "Gi79wr_JeCze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To be ran in Colab notebook, I don't own a GPU\n",
        "\n",
        "#from CudaNet import Net\n",
        "\n",
        "def testEpochAccuracy(size,accuracy,lr):\n",
        "  testNet.setSize(size)\n",
        "\n",
        "  weightsFile = \"sigmoid-untrained-weights\"\n",
        "  testNet.loadWeights(weightsFile)\n",
        "  testNet.learningRate = numpy.float32(lr)\n",
        "  testNet.copyToDevice()\n",
        "\n",
        "  for epoch in range(1):\n",
        "    print(\"\\nEPOCH\",epoch,\"\\n\")\n",
        "    start_time = time.time()\n",
        "    for i in range(len(img_train)):\n",
        "      trainImg32 = img_train[i].astype(numpy.float32)\n",
        "      cuda.memcpy_htod(img_gpu,trainImg32)\n",
        "\n",
        "      testNet.forward(img_gpu)\n",
        "      testNet.getLoss(label_train[i])\n",
        "      checkTrainingAnswer(testNet,label_train[i])\n",
        "      testNet.backward()\n",
        "      \n",
        "      if i % batchSize == 0 or i == (len(img_train) - 1):\n",
        "        testNet.optimize()\n",
        "        testNet.zero_grad()\n",
        "\n",
        "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "    cuda.memcpy_dtoh(training_correct,training_correct_gpu)\n",
        "    reset_values(training_correct_gpu,numpy.int32(1),block=(1,1,1))\n",
        "    print(\"train dataset: correct = \",(training_correct[0]/len(img_train)))\n",
        "    test(testNet)\n",
        "  assert test_correct[0]/len(img_test) == accuracy,\"test accuracy has changed.\"\n",
        "  testNet.free()\n",
        "\n",
        "\n",
        "testNet = Net()\n",
        "\n",
        "batchSize = 1\n",
        "testEpochAccuracy([784,16,10],0.8948,0.1)\n",
        "testEpochAccuracy([784,4,10],0.7046,0.1)\n",
        "testEpochAccuracy([784,16,10,10],0.8181,1)\n",
        "testEpochAccuracy([784,1200,10],0.8525,0.1)\n",
        "batchSize = 10\n",
        "testEpochAccuracy([784,16,10],0.8614,0.1)\n",
        "batchSize = 1"
      ],
      "metadata": {
        "id": "VqOFr1ofidzw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}